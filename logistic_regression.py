# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RyjERph85JrlOyaNxtbzSl5d5wuH72-_
"""

# Imports

import numpy as np
import pandas as pd
import random
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# from google.colab import drive
# drive.mount('/content/drive')

# path ="/content/drive/MyDrive/2025-2/Machine Learning/proyecto1/"
# train_df = pd.read_csv(path + "datos_entrenamiento_riesgo.csv")
# test_df = pd.read_csv(path +  "datos_prueba_riesgo.csv")

train_df = pd.read_csv('datos_entrenamiento_riesgo.csv')
test_df = pd.read_csv('datos_prueba_riesgo.csv')

# EDA
def impute_variables(train_df, test_df):
    categorical_encoded = ['sector_laboral', 'tipo_vivienda', 'nivel_educativo', 'estado_civil']
    for col in categorical_encoded:
        mode_train = train_df[col].mode()[0]
        train_df[col].fillna(mode_train, inplace=True)
        test_df[col].fillna(mode_train, inplace=True)

    numerical = ['porcentaje_utilizacion_credito', 'proporcion_pagos_a_tiempo',
                'residencia_antiguedad_meses', 'lineas_credito_abiertas']
    for col in numerical:
        median_train = train_df[col].median()
        train_df[col].fillna(median_train, inplace=True)
        test_df[col].fillna(median_train, inplace=True)
    return train_df, test_df

def normalize(x_train, x_test):
    scaler = StandardScaler()
    x_train_norm = scaler.fit_transform(x_train)
    x_test_norm = scaler.transform(x_test)
    return x_train_norm, x_test_norm

def add_bias(x):
    return np.column_stack([np.ones(x.shape[0]), x])

def encode_labels(y):
    label_map = {'Bajo': 0, 'Medio': 1, 'Alto': 2}
    return y.map(label_map).values

# Preparar datasets con imputación
train_imputed, test_imputed = impute_variables(train_df.copy(), test_df.copy())

# Separar features y targets
X_train_imputed = train_imputed.drop('nivel_riesgo', axis=1)
X_test_imputed = test_imputed.drop('nivel_riesgo', axis=1)
y_train_imputed = train_imputed['nivel_riesgo']
y_test_imputed = test_imputed['nivel_riesgo']

# Normalize
X_train_norm, X_test_norm = normalize(X_train_imputed, X_test_imputed)

# Agregar Bias
X_train = add_bias(X_train_norm)
X_test = add_bias(X_test_norm)

# Convertir etiquetas
y_train = encode_labels(y_train_imputed)
y_test = encode_labels(y_test_imputed)

# Modelo
def h(x,w):
  return np.dot(x, w)

def s(x,w):
  return 1/ (1 + np.exp(-h(x, w)))

def Loss_function(x, y, w):
  epsilon = 1e-15
  n = x.shape[0]
  sum = np.sum(y * np.log(s(x, w) + epsilon)  +
                 (1 - y) * np.log(epsilon + 1 - s(x, w)))
  return (-1/n) * sum

def Derivatives(x,y,w):
  sum = np.dot(-x.T,y - (s(x,w)))
  n = x.shape[0]
  return (1/n) * sum

def change_parameters(w, derivatives, alpha):
  return w - alpha * derivatives

def training(x_train,y_train, epochs, alpha, x_test, y_test):
  print(len(x_train))
  w_train=np.ones(x_train.shape[1])
  LossTrain = []
  LossTest = []
  for i in range(epochs):
    L_Train =  Loss_function(x_train,y_train,w_train)
    L_Test = Loss_function(x_test, y_test, w_train)
    dw = Derivatives(x_train,y_train,w_train)
    w_train =  change_parameters(w_train, dw, alpha)
    if i % 100 == 0:
      print("L_Train:", L_Train)
    LossTrain.append(L_Train)
    LossTest.append(L_Test)
  return w_train,LossTrain, LossTest

def Testing(x_test, y_test,w):
   y_pred = s(x_test,w)
   y_pred = np.round(y_pred)
   correctos = np.sum(y_pred == y_test)
   print(f"Número de datos correctos:{correctos}")
   porc_aciertos= (correctos/len(y_test))*100
   print(f"Porcentaje de aciertos:{porc_aciertos}%")
   print(f"Porcentaje de error:{100-porc_aciertos}%")

def one_vs_all_training(X_train, y_train, X_test, y_test, epochs, alpha):
    models = {}
    class_names = ['Bajo', 'Medio', 'Alto']

    for i, class_name in enumerate(class_names):
        print(f"\n--- Entrenando modelo {class_name} vs Resto ---")

        y_binary = (y_train == i).astype(int)
        y_test_binary = (y_test == i).astype(int)

        print(f"Distribución {class_name}: {np.sum(y_binary)}/{len(y_binary)}")

        # Entrenar modelo binario
        w_model, loss_train, loss_test = training(
            X_train, y_binary, epochs, alpha, X_test, y_test_binary
        )

        models[i] = w_model

    return models

def predict_one_vs_all(X_test, models):
    n_samples = X_test.shape[0]
    n_classes = len(models)

    probabilities = np.zeros((n_samples, n_classes))

    for i, w_model in models.items():
        probabilities[:, i] = s(X_test, w_model)

    y_pred = np.argmax(probabilities, axis=1)

    return y_pred, probabilities

# Matriz de confusión
def analyze_results(y_true, y_pred, title="Matriz de Confusión"):
  cm = confusion_matrix(y_true, y_pred)
  class_names = ['Bajo', 'Medio', 'Alto']

  cm_decimal = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

  plt.figure(figsize=(6, 4))
  sns.heatmap(cm_decimal, annot=True, fmt='.3f', cmap='Blues',
              xticklabels=class_names, yticklabels=class_names, cbar=False)

  plt.title(title)
  plt.tight_layout()
  filename = f'{title.replace(" ", "_").replace("-", "_")}_decimal.png'
  plt.savefig(filename, dpi=300, bbox_inches='tight')
  plt.close()
  print(f"Gráfico guardado como: {filename}")

def compute_accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred) * 100


def main():
  # Correr el modelo sin Feature Selection o Feature Extraction

  # Definir Hiperparametros
  alpha = 0.9
  epochs = 10000

  # Entrenar el modelo
  w_train, loss_train, loss_test = training(
      X_train, y_train, epochs, alpha, X_test, y_test
  )

  models_ova = one_vs_all_training(
          X_train, y_train,
          X_test, y_test,
          epochs, alpha
      )

  y_pred_ova, probs_ova = predict_one_vs_all(X_test, models_ova)

  # Analizar resultados
  accuracy_ova = compute_accuracy(y_test, y_pred_ova)
  print(f"\nAccuracy One-vs-All: {accuracy_ova:.2f}%")

  analyze_results(y_test, y_pred_ova, "One-vs-All Logistic Regression")

if __name__ == '__main__':
    main()