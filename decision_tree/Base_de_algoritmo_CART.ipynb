{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df5d276",
   "metadata": {},
   "source": [
    "# Árboles de decisión y el algoritmo CART\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "Los **árboles de decisión** constituyen uno de los métodos más interpretables y ampliamente utilizados en el aprendizaje automático supervisado.  \n",
    "Su estructura jerárquica permite dividir progresivamente el espacio de atributos en regiones homogéneas respecto a la variable de salida, generando reglas de decisión de fácil comprensión.  \n",
    "\n",
    "Cada división (o *split*) en el árbol se selecciona con el objetivo de reducir al máximo la heterogeneidad de las clases en los nodos hijos.  \n",
    "Para lograrlo, se emplean métricas de **impureza**, como la **entropía** o el **índice de Gini**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f96b85",
   "metadata": {},
   "source": [
    "## 2. Medidas de impureza: Entropía vs. Gini\n",
    "\n",
    "- **Entropía** (usada en ID3/C4.5):\n",
    "  $$\n",
    "  H(t) = -\\sum_{j=1}^K p(j\\mid t)\\,\\log_2 p(j\\mid t)\n",
    "  $$\n",
    "\n",
    "  mide el desorden en el nodo. Valores altos indican una distribución equilibrada de clases.\n",
    "\n",
    "- **Índice de Gini** (usado en CART):  \n",
    "  $$\n",
    "  H(t) = -\\sum_{j=1}^K p(j\\mid t)\\,\\log_2 p(j\\mid t)\n",
    "  $$\n",
    "\n",
    "  mide la probabilidad de clasificación errónea al asignar etiquetas al azar según la distribución de clases.  \n",
    "\n",
    "Ambas métricas son coherentes y producen resultados similares en la práctica.  \n",
    "Sin embargo, el índice de Gini es **computacionalmente más eficiente**, al no requerir el cálculo de logaritmos, y tiende a generar divisiones algo más equilibradas en cuanto al tamaño de los nodos.  \n",
    "Por estas razones, **CART adopta Gini como medida por defecto**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f3336",
   "metadata": {},
   "source": [
    "## 3. De binario a multiclase\n",
    "\n",
    "En sus orígenes, los árboles de decisión se aplicaron principalmente a problemas de **clasificación binaria**.  \n",
    "No obstante, la definición del índice de Gini es lo suficientemente general para extenderse de manera inmediata al caso de **múltiples clases**:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(t) = 1 - \\sum_{j=1}^K p(j \\mid t)^2, \\quad K \\geq 2\n",
    "$$\n",
    "\n",
    "\n",
    "De esta forma, la estructura binaria del árbol (dos nodos hijos por división) se mantiene, pero los nodos pueden contener observaciones de varias clases simultáneamente.  \n",
    "El criterio de división sigue siendo válido en cualquier escenario: minimizar la impureza ponderada de los nodos hijos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f5028",
   "metadata": {},
   "source": [
    "## 4. El algoritmo CART\n",
    "\n",
    "El método **CART (Classification and Regression Trees)** formaliza este proceso en cuatro pasos principales:\n",
    "\n",
    "\n",
    "### 1. **Selección de variable y umbral**\n",
    "\n",
    "Para cada atributo, se ordenan sus valores y se prueban como candidatos de corte los puntos medios entre observaciones consecutivas distintas:\n",
    "\n",
    "$$\n",
    "c_i = \\frac{x_{(i)} + x_{(i+1)}}{2}, \\quad i = 1, \\dots, n-1\n",
    "$$\n",
    "\n",
    "\n",
    "### 2. **Evaluación del split**\n",
    "\n",
    "Cada corte induce una partición \\((t_L, t_R)\\).  \n",
    "La calidad del split se evalúa mediante la impureza ponderada:\n",
    "\n",
    "$$\n",
    "I(s,t) = \\frac{n_L}{n}\\,\\text{Gini}(t_L) + \\frac{n_R}{n}\\,\\text{Gini}(t_R)\n",
    "$$\n",
    "\n",
    "\n",
    "### 3. **Elección del mejor split**\n",
    "\n",
    "El par (atributo, umbral) que minimiza la impureza se selecciona como división óptima:\n",
    "\n",
    "$$\n",
    "s^* = \\arg\\min_{s}\\; I(s,t)\n",
    "$$\n",
    "\n",
    "\n",
    "### 4. **Recursividad y detención**\n",
    "\n",
    "El proceso se repite en los nodos hijos hasta cumplir un criterio de parada: nodo puro, tamaño mínimo de muestras o profundidad máxima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbfb43f",
   "metadata": {},
   "source": [
    "## Búsqueda de umbral en variables continuas\n",
    "\n",
    "Para variables numéricas, CART **ordena los valores de la característica** y evalúa cortes posibles entre observaciones consecutivas distintas.  \n",
    "Cada corte se coloca en el **punto medio** entre dos valores distintos.\n",
    "\n",
    "$$\n",
    "c = \\frac{x_{(i)} + x_{(i+1)}}{2}, \n",
    "\\quad \\text{con } n-1 \\text{ posibles cortes entre valores consecutivos}\n",
    "$$\n",
    "\n",
    "- Si los valores son iguales, no se prueba un corte (no aporta nada).  \n",
    "- Si los valores difieren, el corte se evalúa justo en el medio.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff8f53",
   "metadata": {},
   "source": [
    "### Clase `Nodo`\n",
    "\n",
    "Representa un nodo del árbol, ya sea interno o una hoja.  \n",
    "Contiene la información local y los enlaces a subárboles.\n",
    "\n",
    "- **Atributos principales:**\n",
    "  - `X`, `Y`: subconjunto de datos y etiquetas en el nodo.\n",
    "  - `feature_index`, `threshold`: variable y umbral elegidos para dividir.\n",
    "  - `left`, `right`: nodos hijos izquierdo y derecho.\n",
    "  - `label`: clase mayoritaria cuando el nodo es hoja.\n",
    "\n",
    "- **Funciones clave:**\n",
    "  - `is_terminal()`: determina si el nodo debe ser una hoja  \n",
    "    (cuando no hay datos o todas las observaciones son de la misma clase).\n",
    "  - `gini_counts(counts, n)`: calcula la impureza de Gini de un nodo a partir de conteos de clases:  \n",
    "    $$\n",
    "    \\text{Gini}(t) = 1 - \\sum_{j=1}^K p(j \\mid t)^2\n",
    "    $$\n",
    "\n",
    "  - `best_split(min_samples_leaf)`: busca la mejor división en el nodo actual.  \n",
    "    - Ordena los valores de cada característica.  \n",
    "    - Evalúa cortes en los puntos medios entre valores consecutivos distintos.  \n",
    "    - Calcula la **impureza ponderada**:  \n",
    "      $$\n",
    "      I(s,t) = \\frac{n_L}{n}\\,\\text{Gini}(t_L) + \\frac{n_R}{n}\\,\\text{Gini}(t_R)\n",
    "      $$\n",
    "    - Devuelve la mejor variable y umbral encontrados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c14f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nodo:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.label = None\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return len(self.Y) == 0 or len(set(self.Y)) == 1\n",
    "\n",
    "    def gini_counts(self, counts, n):\n",
    "        # counts: vector con conteos por clase (long K)\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        p = counts / n\n",
    "        return 1.0 - np.sum(p * p)\n",
    "\n",
    "    def best_split(self, min_samples_leaf=1):\n",
    "        n, d = self.X.shape\n",
    "        if n <= 1:\n",
    "            return None\n",
    "\n",
    "        # Mapear etiquetas a 0..K-1 una sola vez por nodo\n",
    "        classes, y_idx = np.unique(self.Y, return_inverse=True)\n",
    "        K = len(classes)\n",
    "\n",
    "        best_gini = np.inf\n",
    "        best = None\n",
    "\n",
    "        # Conteos totales a la derecha (antes de partir)\n",
    "        total_right = np.bincount(y_idx, minlength=K).astype(np.int64)\n",
    "\n",
    "        for j in range(d):\n",
    "            # Ordenar por la columna j\n",
    "            order = np.argsort(self.X[:, j], kind=\"mergesort\")\n",
    "            xj = self.X[order, j]\n",
    "            yj = y_idx[order]\n",
    "\n",
    "            left_counts = np.zeros(K, dtype=np.int64)\n",
    "            right_counts = total_right.copy()\n",
    "\n",
    "            # Recorremos posibles cortes entre i e i+1\n",
    "            for i in range(n - 1):\n",
    "                c = yj[i]\n",
    "                left_counts[c]  += 1\n",
    "                right_counts[c] -= 1\n",
    "\n",
    "                # Saltar si los valores son iguales \n",
    "                if xj[i] == xj[i + 1]:\n",
    "                    continue\n",
    "\n",
    "                nL = i + 1\n",
    "                nR = n - nL\n",
    "                # hojas mínimas\n",
    "                if nL < min_samples_leaf or nR < min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                gL = self.gini_counts(left_counts, nL)\n",
    "                gR = self.gini_counts(right_counts, nR)\n",
    "                g  = (nL * gL + nR * gR) / n  # PONDERADO\n",
    "\n",
    "                if g < best_gini:\n",
    "                    thr = (xj[i] + xj[i + 1]) / 2.0  # punto medio\n",
    "                    best_gini = g\n",
    "                    best = (j, thr)\n",
    "\n",
    "        return best\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6863c",
   "metadata": {},
   "source": [
    "\n",
    "### Clase `DT` (Decision Tree)\n",
    "\n",
    "Gestiona el árbol completo y su construcción.\n",
    "\n",
    "- **Parámetros de control:**\n",
    "  - `max_depth`: profundidad máxima permitida.\n",
    "  - `min_samples_split`: mínimo de observaciones para intentar un split.\n",
    "  - `min_samples_leaf`: mínimo de observaciones en una hoja.\n",
    "\n",
    "- **Funciones clave:**\n",
    "  - `fit(X, Y)`: inicia el entrenamiento construyendo la raíz con todos los datos.\n",
    "  - `_grow(X, Y, depth)`: construcción recursiva del árbol.  \n",
    "    - Evalúa criterios de parada (nodo terminal, profundidad máxima, tamaño mínimo).  \n",
    "    - Si no se detiene, aplica `best_split()` del nodo actual.  \n",
    "    - Divide el dataset en izquierdo y derecho, y llama recursivamente.  \n",
    "  - `_predict_one(node, x)`: predice la clase de una observación recorriendo el árbol desde la raíz hasta una hoja.\n",
    "  - `predict(X)`: aplica `_predict_one` a todas las observaciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67bd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = self._grow(X, Y, depth=0)\n",
    "\n",
    "    def _grow(self, X, Y, depth):\n",
    "        node = Nodo(X, Y)\n",
    "\n",
    "        # parada\n",
    "        if (node.is_terminal() or\n",
    "            (self.max_depth is not None and depth >= self.max_depth) or\n",
    "            len(Y) < self.min_samples_split):\n",
    "            node.label = Counter(Y).most_common(1)[0][0] if len(Y) else None\n",
    "            return node\n",
    "\n",
    "        split = node.best_split(min_samples_leaf=self.min_samples_leaf)\n",
    "        if split is None:\n",
    "            node.label = Counter(Y).most_common(1)[0][0]\n",
    "            return node\n",
    "\n",
    "        j, t = split\n",
    "        node.feature_index, node.threshold = j, t\n",
    "\n",
    "        left_mask  = X[:, j] <= t\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        node.left  = self._grow(X[left_mask],  Y[left_mask],  depth + 1)\n",
    "        node.right = self._grow(X[right_mask], Y[right_mask], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict_one(self, node, x):\n",
    "        if node.label is not None or node.is_terminal():\n",
    "            return node.label\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_one(node.left, x)\n",
    "        else:\n",
    "            return self._predict_one(node.right, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(self.root, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502d3f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Conexión con la teoría de CART\n",
    "\n",
    "1. **Medida de impureza:**  \n",
    "   Se implementa con `gini_counts`, usando la fórmula clásica de Gini.\n",
    "\n",
    "2. **Selección de split:**  \n",
    "   `best_split` examina todas las variables y candidatos de corte.  \n",
    "   Elige la división que **minimiza la impureza ponderada**.\n",
    "\n",
    "3. **Construcción recursiva:**  \n",
    "   `_grow` refleja la naturaleza recursiva del algoritmo: cada nodo se expande hasta cumplir un criterio de detención.\n",
    "\n",
    "4. **Predicción:**  \n",
    "   `_predict_one` y `predict` reproducen el proceso de clasificación: un ejemplo desciende por el árbol comparando sus atributos con los umbrales hasta llegar a una hoja.\n",
    "\n",
    "\n",
    "### Resumen\n",
    "\n",
    "- **`Nodo`** encapsula la lógica local (datos, pureza, mejor división).  \n",
    "- **`DT`** organiza el crecimiento global del árbol y el proceso de predicción.  \n",
    "- El flujo completo replica el **algoritmo CART con índice de Gini**, extendido naturalmente al caso multiclase, pues la suma en el cálculo de Gini incluye todas las clases posibles.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355a514",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
